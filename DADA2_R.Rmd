---
title: "DADA2 IN R"
author: "Steph"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    theme: readable
    highlight: tango
    toc: true
    toc_depth: 2
    toc_float: true
---

# DADA2 IN R

## OPTION 1 : MERGED SEQUENCES

- Load libraries and check versions

```{r, eval=FALSE}
library(dada2)
packageVersion("dada2")
library(ShortRead)
packageVersion("ShortRead")
library(Biostrings)
packageVersion("Biostrings")
```

- Path where the sequences are saved
```{r, eval=FALSE}
path <- "../ITS_corredor/HN00174206/data_REANAL_2024_LAST/ITS_STANDALONE/ITSxpress_merged/"  ## CHANGE ME to the directory containing the fastq files.
head(list.files(path))
```

- List of files
```{r, eval=FALSE}
fnFs <- sort(list.files(path, pattern = "_L001_trimmed_reads.fastq.gz", full.names = TRUE))
```
- Primers
```{r, eval=FALSE}
FWD <- "AACTTTYRRCAAYGGATCWCT"  ## CHANGE ME to your forward primer sequence
REV <- "AGCCTCCGCTTATTGATATGCTTAART"  ## CHANGE ME...
```
- Check for primers (not must be present because itsxpress)
```{r, eval=FALSE}
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = Biostrings::reverse(dna),
        RevComp = Biostrings::reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
```

```{r, eval=FALSE}
fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) 
#filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)
filterAndTrim(fnFs, fnFs.filtN, maxN = 0, multithread = TRUE)
```

```{r, eval=FALSE}
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]),
      #FWD.ReverseReads = sapply(FWD.orients,primerHits, fn = fnRs.filtN[[1]]))
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]))
      #REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))
```

- Quality plot
```{r, eval=FALSE}
plotQualityProfile(fnFs[1:2])
#plotQualityProfile(cutRs[1:2])

```
- Filter by quality

```{r, eval=FALSE}
filtFs <- file.path(path, "filtered", basename(fnFs))

```

```{r, eval=FALSE}
out <- filterAndTrim(fnFs, filtFs, maxN = 0, maxEE = 2, truncQ = 2,
    minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = TRUE)  # on windows, set multithread = FALSE
head(out)
```
- Model of lear Errors
```{r, eval=FALSE}
errF <- learnErrors(filtFs, multithread = TRUE)
#plotErrors(errF, nominalQ = TRUE)

```
- Dereplication
```{r, eval=FALSE}
derepFs <- derepFastq(filtFs, verbose = TRUE)
```
- Inference of ASVs
```{r, eval=FALSE}
dadaFs <- dada(derepFs, err = errF, multithread = TRUE)
dadaFs2 <- dada(derepFs, err = errF, multithread = TRUE,  OMEGA_A=1e-40, pool="pseudo" )

```


- Sequences table

```{r, eval=FALSE}
seqtab <- makeSequenceTable(dadaFs)
seqtab2 <- makeSequenceTable(dadaFs2)
dim(seqtab)
```


- chimera removal
```{r, eval=FALSE}
seqtab_nochim <- removeBimeraDenovo(seqtab, method = "consensus", multithread = T, 
                                    verbose = T)
seqtab_nochim2 <- removeBimeraDenovo(seqtab2, method = "consensus", multithread = T, 
                                    verbose = T)
dim(seqtab_nochim)
```

-  *Denoising* statistics
```{r, eval=FALSE}
getN <-function(x) sum(getUniques(x))
# ?getUniques
stats <- cbind(out, sapply(dadaFs, getN), 
               rowSums(seqtab_nochim2))

colnames(stats) <- c("input", "filtered", "denoisedF",  "nonchim")

head(stats)
```

```{r, eval=FALSE}
stats2 <- cbind(out, sapply(dadaFs2, getN), 
               rowSums(seqtab_nochim2))

colnames(stats2) <- c("input", "filtered", "denoisedF",  "nonchim")

head(stats2)
```


- Sequences statistics (chatGPT)
```{r, eval=FALSE}
library(tidyverse)
# Obtener los nombres de las columnas de 'df'
seqs <- colnames(seqtab)

# Función para contar letras en una cadena
contar_letras <- function(cadena) {
  str_count(cadena, "[A-Za-z]")
}

# Aplicar la función de conteo a cada elemento de 'nombres_columnas'
conteo_letras <- map_int(seqs, ~ contar_letras(.x))

#print(conteo_letras)
min(conteo_letras)
max(conteo_letras)
mean(conteo_letras)

```


```{r, echo=FALSE, eval=FALSE}
#rename data
seqtab_final <- as.data.frame(seqtab_nochim) %>%
  rownames_to_column(var = "ids") %>%
  mutate(ids = str_extract(ids, "[0-9]+[A-Z]+")) %>%
  column_to_rownames(var = "ids")

seqtab_final2 <- as.data.frame(seqtab_nochim2) %>%
  rownames_to_column(var = "ids") %>%
  mutate(ids = str_extract(ids, "[0-9]+[A-Z]+")) %>%
  column_to_rownames(var = "ids")

```


- Assign taxonomy

```{r, eval=FALSE}
#----------Asignacion taxonomica-----------
ruta_clasificador <- "../ITS_corredor/HN00174206/data_REANAL_2024_LAST/sh_general_release_dynamic_18.07.2023.fasta" # de la página de UNITE
taxa <- assignTaxonomy(seqtab_nochim, ruta_clasificador, multithread = TRUE, tryRC = TRUE)
taxa2 <- assignTaxonomy(seqtab_nochim2, ruta_clasificador, multithread = TRUE, tryRC = TRUE)

# Visualizar lo que se genero despues de la asignacion
taxa_print <- taxa
rownames(taxa_print) <- NULL

head(taxa_print)
dim(taxa_print)
```

- Export data

```{r, eval=FALSE}
save( seqtab_nochim2, taxa2,
      file = "dada2_results2.RData")
write.csv(taxa, "taxonomy.csv")
write.csv(seqtab_final, "table_final.csv", row.names = TRUE)
write_tsv(seqtab_final %>% rownames_to_column("#SampleID"), "table_final.txt")

write.csv(stats, "stats.csv")

#remotes::install_github("vmikk/metagMisc")
library(metagMisc)
dada_to_fasta(seqtab = seqtab, out = "seqtab.fasta")
dada_to_fasta(seqtab = seqtab2, out = "seqtab2.fasta")



```

```{r, echo=FALSE, eval=FALSE}
library("Biostrings")

seqs_data = readDNAStringSet("seqtab2.fasta")
seq_name = names(seqs_data)
sequence = paste(seqs_data)
df <- data.frame(seq_name, sequence)
df= df %>% separate(seq_name, into = c("ids", "length"), sep = ";")
taxo <- df %>% inner_join(as.data.frame(taxa2) %>% rownames_to_column(var = "sequence")) %>% 
  unite("Taxon", Kingdom:Species, sep = ";") %>% dplyr::select("Feature ID"=ids, Taxon)

data<- seqtab_final2 %>% t() %>% as.data.frame() %>% rownames_to_column(var = "sequence") %>% inner_join(df) %>% dplyr::select("#OTU ID"=ids, "111D":"623R" ) 
write_tsv(data, "tabla2.txt")
write_tsv(taxo, "taxonomy2_final.txt")


```


## OPTION 2 : UNMERGED SEQUENCES

- Load libraries and check versions

```{r, eval=FALSE}
library(dada2)
packageVersion("dada2")
library(ShortRead)
packageVersion("ShortRead")
library(Biostrings)
packageVersion("Biostrings")
```

- Path where the sequences are saved
```{r, eval=FALSE}
path <- "../ITS_corredor/HN00174206/data_REANAL_2024_LAST/ITS_STANDALONE/"  ## CHANGE ME to the directory containing the fastq files.
head(list.files(path))
```

- List of files
```{r, eval=FALSE}
fnFs <- sort(list.files(path, pattern = "_R1.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2.fastq.gz", full.names = TRUE))

```
- Primers
```{r, eval=FALSE}
FWD <- "AACTTTYRRCAAYGGATCWCT"  ## CHANGE ME to your forward primer sequence
REV <- "AGCCTCCGCTTATTGATATGCTTAART"  ## CHANGE ME...
```
- Check for primers (not must be present because itsxpress)
```{r, eval=FALSE}
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = Biostrings::reverse(dna),
        RevComp = Biostrings::reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
```

```{r, eval=FALSE}
fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filtered files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)
```

```{r, eval=FALSE}
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), FWD.ReverseReads = sapply(FWD.orients,
    primerHits, fn = fnRs.filtN[[1]]), REV.ForwardReads = sapply(REV.orients, primerHits,
    fn = fnFs.filtN[[1]]), REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))
```


- remove primers
```{r, eval=FALSE}
#cutadapt <- "C:/Users/HP/AppData/Local/Programs/Python/Python312/Scripts/cutadapt.exe" # CHANGE ME to the cutadapt path on your machine
#system2(cutadapt, args = "--version") # Run shell commands from R

# Definir la ruta al lanzador de Python
python_launcher <- "py.exe"

# Ejecutar cutadapt desde R
output <- system2(python_launcher, args = c("-m", "cutadapt", "--version"), stdout = TRUE)
print(output)


path.cut <- file.path(path, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC) 
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC)
```

```{r, eval=FALSE}
# Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(python_launcher, args = c("-m", "cutadapt",R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             fnFs.filtN[i], fnRs.filtN[i],
                             "--minimum-length=1")) # input files
}
```


```{r, eval=FALSE}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), FWD.ReverseReads = sapply(FWD.orients,
    primerHits, fn = fnRs.cut[[1]]), REV.ForwardReads = sapply(REV.orients, primerHits,
    fn = fnFs.cut[[1]]), REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))



# Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path.cut, pattern = "_R1.fastq.gz", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "_R2.fastq.gz", full.names = TRUE))

# Extract sample names, assuming filenames have format:
get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]
sample.names <- unname(sapply(cutFs, get.sample.name))
head(sample.names)
```


- Quality plot
```{r, eval=FALSE}
plotQualityProfile(cutFs[1:2])
plotQualityProfile(cutRs[1:2])

```
- Filter by quality

```{r, eval=FALSE}
filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))
```

```{r, eval=FALSE}
out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(2, 2), truncQ = 2,
    minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = TRUE)  # on windows, set multithread = FALSE
head(out)
```
- Model of lear Errors
```{r, eval=FALSE}
errF <- learnErrors(filtFs, multithread = TRUE)
errR <- learnErrors(filtRs, multithread = TRUE)

#plotErrors(errF, nominalQ = TRUE)

```
- Dereplication
```{r, eval=FALSE}
derepFs <- derepFastq(filtFs, verbose = TRUE)
derepRs <- derepFastq(filtRs, verbose = TRUE)


```
- Inference of ASVs
```{r, eval=FALSE}
dadaFs <- dada(derepFs, err = errF, multithread = TRUE )
dadaRs <- dada(derepRs, err = errF, multithread = TRUE )

```

- Merge
```{r, eval=FALSE}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)

```


- Obtención de tabla de secuencias

```{r, eval=FALSE}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```


- Quimeras removal
```{r, eval=FALSE}
seqtab_nochim <- removeBimeraDenovo(seqtab, method = "consensus", multithread = T, 
                                    verbose = T)
dim(seqtab_nochim)
```

- *Denoising* statistics
```{r, eval=FALSE}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN),
    rowSums(seqtab_nochim))
# If processing a single sample, remove the sapply calls: e.g. replace
# sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```



- Sequences statistics (chatGPT)
```{r, eval=FALSE}
library(tidyverse)
# Obtener los nombres de las columnas de 'df'
seqs <- colnames(seqtab)

# Función para contar letras en una cadena
contar_letras <- function(cadena) {
  str_count(cadena, "[A-Za-z]")
}

# Aplicar la función de conteo a cada elemento de 'nombres_columnas'
conteo_letras <- map_int(seqs, ~ contar_letras(.x))

#print(conteo_letras)
min(conteo_letras)
max(conteo_letras)
mean(conteo_letras)

```


```{r, echo=FALSE, eval=FALSE}
#rename data
seqtab_final <- as.data.frame(seqtab_nochim) %>%
  rownames_to_column(var = "ids") %>%
  mutate(ids = str_extract(ids, "[0-9]+[A-Z]+")) %>%
  column_to_rownames(var = "ids")

seqtab_final2 <- as.data.frame(seqtab_nochim2) %>%
  rownames_to_column(var = "ids") %>%
  mutate(ids = str_extract(ids, "[0-9]+[A-Z]+")) %>%
  column_to_rownames(var = "ids")

```


- Assign taxonomy

```{r, eval=FALSE}
#----------Asignacion taxonomica-----------
ruta_clasificador <- "../ITS_corredor/HN00174206/data_REANAL_2024_LAST/sh_general_release_dynamic_18.07.2023.fasta" # de la página de UNITE
taxa <- assignTaxonomy(seqtab_nochim, ruta_clasificador, multithread = TRUE, tryRC = TRUE)

# Visualizar lo que se genero despues de la asignacion
taxa_print <- taxa
rownames(taxa_print) <- NULL

head(taxa_print)
dim(taxa_print)
```

#### Exportar objetos generados durante el pre-procesamiento 

```{r, eval=FALSE}
save( seqtab_nochim, taxa, 
      file = "dada2_results_paired_final.RData")
write_tsv(seqtab_final %>% rownames_to_column("#SampleID"), "table3_no_hasids.txt")


#remotes::install_github("vmikk/metagMisc")
library(metagMisc)
dada_to_fasta(seqtab = seqtab, out = "seqtab3.fasta")

seqtab_final <- as.data.frame(seqtab_nochim) %>%
  rownames_to_column(var = "ids") %>%
  mutate(ids = str_extract(ids, "[0-9]+[A-Z]+")) %>%
  column_to_rownames(var = "ids")

```

```{r, echo=FALSE, eval=FALSE}
library("Biostrings")

seqs_data = readDNAStringSet("seqtab3.fasta")
seq_name = names(seqs_data)
sequence = paste(seqs_data)
df <- data.frame(seq_name, sequence)
df= df %>% separate(seq_name, into = c("ids", "length"), sep = ";")
taxo <- df %>% inner_join(as.data.frame(taxa) %>% rownames_to_column(var = "sequence")) %>% 
  unite("Taxon", Kingdom:Species, sep = ";") %>% dplyr::select("Feature ID"=ids, Taxon)


data<- seqtab_final %>% t() %>% as.data.frame() %>% rownames_to_column(var = "sequence") %>% inner_join(df) %>% dplyr::select("#OTU ID"=ids, "111D":"623R" ) 
write_tsv(data, "tabla3.txt")
write_tsv(taxo, "taxonomy3_final.txt")


```